{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cybOQrlMupPM","outputId":"df3ea35c-9b74-41bd-d349-0537c80b3b02"},"outputs":[{"output_type":"stream","name":"stderr","text":["The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n","The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","Setting `pad_token_id` to `eos_token_id`:0 for open-end generation.\n"]}],"source":["import torch\n","from transformers import GPT2LMHeadModel, GPT2Tokenizer\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model_name = \"nferruz/ProtGPT2\"\n","saved_model_path = \"/content/drive/MyDrive/s1116049/EFFLUX_NEW_WORK/Model_ABC\"\n","tokenizer = GPT2Tokenizer.from_pretrained(model_name, padding_side='left')\n","model = GPT2LMHeadModel.from_pretrained(saved_model_path).to(device)\n","tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n","\n","model.eval()\n","sequence = (\"It looks like the error is occurring because the sequence IDs extracted from the CSV file include the header ro\")\n","\n","sequences = []\n","\n","for _ in range(30):\n","    sequence = tokenizer.decode(model.generate(max_length=1800, do_sample=True, top_k=950,temperature=0.8, repetition_penalty=1.2, num_return_sequences=1, eos_token_id=0)[0])\n","    sequences.append(sequence)\n","\n","\n","for seq in sequences:\n","    print(seq)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2roYQ0Druzbc"},"outputs":[],"source":["\n","import math\n","# Set the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = model.to(device)\n","\n","def calculatePerplexity(sequence, model, tokenizer):\n","    input_ids = torch.tensor(tokenizer.encode(sequence)).unsqueeze(0)\n","    input_ids = input_ids.to(device)\n","    with torch.no_grad():\n","        outputs = model(input_ids, labels=input_ids)\n","    loss, logits = outputs[:2]\n","    return math.exp(loss)\n","\n","# Set the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","# Iterate over all sequences\n","for sequence in sequences:\n","    # Calculate perplexity for the sequence\n","    ppl = calculatePerplexity(sequence, model, tokenizer)\n","\n","    print(\"Sequence:\")\n","    print(sequence)\n","    print(\"Perplexity:\", ppl)\n","    print()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ku5xWc0Ju09s"},"outputs":[],"source":["# Define a threshold for perplexity\n","perplexity_threshold = 90.0  # Adjust this value based on your needs\n","\n","# Create a list to store good sequences\n","good_sequences = []\n","\n","# Create a list to store corresponding perplexity values\n","perplexity_scores = []\n","\n","# Iterate over all sequences\n","for sequence in sequences:\n","    # Calculate perplexity for the sequence\n","    ppl = calculatePerplexity(sequence, model, tokenizer)\n","\n","    # Check if the perplexity is below the threshold\n","    if ppl < perplexity_threshold:\n","        good_sequences.append(sequence)\n","        perplexity_scores.append(ppl)\n","\n","for sequence, ppl in zip(good_sequences, perplexity_scores):\n","    print(\"Sequence:\", sequence)\n","    print(\"Perplexity:\", ppl)\n","    print()\n"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[],"gpuType":"T4","mount_file_id":"11hbISJ7kty-B03nN4dr7coBcv_Nufg4m","authorship_tag":"ABX9TyOBrb6SPnssiiuIltvnY7fi"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}